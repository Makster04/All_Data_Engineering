# Step-by-Step using KNN algorithm
---

## ğŸ“Œ **Step-by-Step Explanation**

### 1. **Choose a data point for which you want to predict a label:**
- Imagine you have a dataset of labeled points (some have label A, others have label B), and you want to predict the label of a new, unknown point.

### 2. **Identify the K nearest points around this new point:**
- Choose a value for **K**. Typical choices are small, odd numbers like 1, 3, 5, or 11.
- Calculate the **distance** from your chosen point to all other points. Typically, Euclidean distance is used:
\[
\text{Distance} = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2 + \dots}
\]

### 3. **Predict the label based on these K neighbors:**
- **Classification**: Assign the most common class label among the K neighbors.
- **Regression**: Calculate the average value of the target variable among the K neighbors.

### 4. **(Optional) Use weighted averages:**
- Instead of treating each neighbor equally, assign weights based on distance (closer neighbors have more influence):
  \[
  \text{Weighted Average} = \frac{\sum (\text{value of neighbor}_i \times \frac{1}{\text{distance}_i})}{\sum (\frac{1}{\text{distance}_i})}
  \]

---

## ğŸš€ **Example Illustration (Classification)**:

Suppose you have the following points (two classes: âšªï¸ and ğŸ”µ):

```
(1, 2) ğŸ”µ 
(2, 3) ğŸ”µ
(3, 3) ğŸ”µ
(6, 5) âšªï¸
(7, 7) âšªï¸
(8, 6) âšªï¸
```

Now you have a new point:  
```
(5, 4) â“
```

Let's find its class using **K = 3**.

### Step-by-Step Solution:

**1.** Calculate distance to all points:

| Existing Point | Class | Distance to (5,4)                |
|----------------|-------|----------------------------------|
| (1,2)          | ğŸ”µ    | âˆš((5-1)Â²+(4-2)Â²)= âˆš(16+4)=âˆš20 â‰ˆ4.47|
| (2,3)          | ğŸ”µ    | âˆš((5-2)Â²+(4-3)Â²)= âˆš(9+1)=âˆš10 â‰ˆ3.16 |
| (3,3)          | ğŸ”µ    | âˆš((5-3)Â²+(4-3)Â²)= âˆš(4+1)=âˆš5 â‰ˆ2.24  |
| (6,5)          | âšªï¸    | âˆš((5-6)Â²+(4-5)Â²)= âˆš(1+1)=âˆš2 â‰ˆ1.41  |
| (7,7)          | âšªï¸    | âˆš((5-7)Â²+(4-7)Â²)= âˆš(4+9)=âˆš13â‰ˆ3.61 |
| (8,6)          | âšªï¸    | âˆš((5-8)Â²+(4-6)Â²)= âˆš(9+4)=âˆš13â‰ˆ3.61 |

**2.** Choose the 3 nearest neighbors:
- (6,5) âšªï¸ Distance=1.41  
- (3,3) ğŸ”µ Distance=2.24  
- (2,3) ğŸ”µ Distance=3.16  

**3.** Majority voting among neighbors:
- ğŸ”µ = 2 votes
- âšªï¸ = 1 vote

**Prediction:**  
```
(5,4) â†’ ğŸ”µ
```

---

## ğŸ“ **Example Illustration (Regression)**:

Now imagine you have a regression problem, and your neighbors have numeric values instead:

| Existing Point | Value | Distance to (5,4)|
|----------------|-------|-------------------|
| (1,2)          | 100   | â‰ˆ4.47             |
| (2,3)          | 90    | â‰ˆ3.16             |
| (3,3)          | 80    | â‰ˆ2.24             |
| (6,5)          | 50    | â‰ˆ1.41             |
| (7,7)          | 30    | â‰ˆ3.61             |
| (8,6)          | 20    | â‰ˆ3.61             |

Again, choose **K=3** neighbors closest to (5,4):

- (6,5): value=50
- (3,3): value=80
- (2,3): value=90

Average these values:
\[
\text{Prediction} = \frac{50 + 80 + 90}{3} = \frac{220}{3} â‰ˆ 73.33
\]

---

## ğŸ¯ **Using Weighted Averages**:

You could instead weight by distance (closer neighbors matter more):

\[
\text{Prediction} = \frac{\frac{50}{1.41} + \frac{80}{2.24} + \frac{90}{3.16}}{\frac{1}{1.41} + \frac{1}{2.24} + \frac{1}{3.16}} 
\]

This gives greater emphasis to closer neighbors.

---

## âš–ï¸ **When to Use KNN:**

- **Pros:**
  - Simple and intuitive
  - No assumptions about underlying data distribution
  - Works well for multi-class classification
  
- **Cons:**
  - Computationally intensive with large datasets
  - Sensitive to irrelevant features and scaling

---

That's how the **K-Nearest Neighbors (KNN)** algorithm works in practice!
