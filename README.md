# All_Data_Engineering

Gradient Boosting uses weight to deal with the overfiitng
- Works best on stumps

- bags the data into

Bagging is with replacemtns 
- with replacemnet can be repeats
- without it you cant repeact
  
Adaboosting d (If the weight was one, it would basxially be gradient boosting Adaboost is gradietn boost if Aplhpa =1 ).


Radnom Forest is multiple decision trees


Random Forest uses bagging to work effectivley 
